{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "F8ESUWX_ZNqD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsoWeUrh9SVN",
    "outputId": "c5c51bcc-2f7e-4f6d-db84-cbc0352cfba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3103, 224, 224, 3)\n",
      "(3103, 1)\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "dataset = np.load('/content/drive/My Drive/data.row/ASD2.npz') #Dataset ready in numpy array (removing background, resizing, and transforming into grayscale)\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5USIjzD4ApZj"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "nfolds = 3\n",
    "nEpochs = 30\n",
    "nBatch= 16\n",
    "#inputDim = X.shape[1] # Count of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUFUdomx2W-1",
    "outputId": "f0b9cc1a-1ff4-4378-8a6c-88dedfb2c2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2068, 224, 224, 3) (1035, 224, 224, 3) (2068, 1) (1035, 1)\n",
      "(2069, 224, 224, 3) (1034, 224, 224, 3) (2069, 1) (1034, 1)\n",
      "(2069, 224, 224, 3) (1034, 224, 224, 3) (2069, 1) (1034, 1)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=1).split(X, y) #Cross validation\n",
    "for train, test in kfold:\n",
    "  print(X[train].shape, X[test].shape ,y[train].shape, y[test].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bo_Uj8cjZNTE",
    "outputId": "089ade12-4cab-4949-9c40-92173f14406f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102973440/102967424 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet50 = tf.keras.applications.ResNet50() \n",
    "#resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_1qy5h2dZNNz"
   },
   "outputs": [],
   "source": [
    "x = resnet50.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0niV2DoRZNLf"
   },
   "outputs": [],
   "source": [
    "output = Dense(units=1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XzG0MP68ZNJD"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=resnet50.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NFARn69XZNGR"
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers[:-23]:\n",
    "#   layer.trainable = False\n",
    "\n",
    "for layer in model.layers[175:]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[:175]:\n",
    "    layer.trainable = False\n",
    "#for i, layer in enumerate(model.layers):\n",
    "    #print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wtTrzDGTZNDX"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SFlFS-rP4pKV"
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "recall =[]\n",
    "precision = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "start = time.time()\n",
    "hists_CNN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eHhScNIXazCy"
   },
   "outputs": [],
   "source": [
    "#hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4]}]\n",
    "# set up GridSearchCV()\n",
    "#model_cv = sklearn.model_selection.GridSearchCV(estimator = model, \n",
    "                      #  param_grid = {}, \n",
    "                     #   scoring= 'accuracy', \n",
    "                     #   cv = kfold, \n",
    "                      #  verbose = 2,\n",
    "                      #  return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTP3sLm8gfu7"
   },
   "outputs": [],
   "source": [
    "#این قسمت رو از کگل برای انجام کافولد پیدا کردم ولی هنوز نتونستم باهاش به نتیجه برسم.البته که یکم کدش برام پیچیده هست.\n",
    "#from tensorflow.keras.applications import DenseNet201\n",
    "\n",
    "#def get_model():\n",
    "    \n",
    "    with strategy.scope():\n",
    "        rnet = DenseNet201(\n",
    "            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        # trainable rnet\n",
    "        rnet.trainable = True\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            rnet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cross_validate(folds = 4):\n",
    "    histories = []\n",
    "    models = []\n",
    "    #early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 4)\n",
    "    #kfold = KFold(folds, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n",
    "        train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES']), labeled = True)\n",
    "        val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES']), labeled = True, ordered = True)\n",
    "        checkpoint_name = f'model_fold_{fold + 1}' + '.h5'\n",
    "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, save_best_only = True, save_weights_only = True)\n",
    "        model = get_model()\n",
    "        history = model.fit(\n",
    "        get_training_dataset(train_dataset), \n",
    "        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "        epochs = EPOCHS,\n",
    "        callbacks = [lr_callback, early_stopping, model_checkpoint],\n",
    "        validation_data = get_validation_dataset(val_dataset)\n",
    "        )\n",
    "        print('Load best weights for model prediction')\n",
    "        model.load_weights(checkpoint_name)\n",
    "        models.append(model)\n",
    "        histories.append(history)\n",
    "        \n",
    "    return histories, models\n",
    "\n",
    "def train_and_predict(folds = 4):\n",
    "    test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "    test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "    print('Start training {} models'.format(folds))\n",
    "    histories, models = train_cross_validate(folds = folds)\n",
    "    print('Computing predictions...')\n",
    "    # get the mean probability of the folds models\n",
    "    probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    print('Generating submission.csv file...')\n",
    "    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n",
    "    return histories, models\n",
    "    \n",
    "# run train and predict\n",
    "histories, models = train_and_predict(folds = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Q64iTKljZpcr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1zKlRQ4ZNAr",
    "outputId": "fb3fdcbe-d941-4977-f3c8-33a377dde965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "104/104 - 6s - loss: 0.6893 - accuracy: 0.6816 - val_loss: 1.2064 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "104/104 - 4s - loss: 0.6456 - accuracy: 0.6985 - val_loss: 2.2023 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "104/104 - 4s - loss: 0.6630 - accuracy: 0.6912 - val_loss: 1.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "104/104 - 4s - loss: 0.6439 - accuracy: 0.6985 - val_loss: 0.6381 - val_accuracy: 0.9952\n",
      "Epoch 5/30\n",
      "104/104 - 4s - loss: 0.6631 - accuracy: 0.6659 - val_loss: 1.9981 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "104/104 - 4s - loss: 0.6836 - accuracy: 0.6931 - val_loss: 1.1282 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "104/104 - 4s - loss: 0.7145 - accuracy: 0.6453 - val_loss: 0.4007 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "104/104 - 4s - loss: 0.6652 - accuracy: 0.6737 - val_loss: 1.8835 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "104/104 - 4s - loss: 0.6461 - accuracy: 0.6858 - val_loss: 1.0731 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "104/104 - 4s - loss: 0.6242 - accuracy: 0.7208 - val_loss: 2.0949 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "104/104 - 4s - loss: 0.6507 - accuracy: 0.6798 - val_loss: 1.8610 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "104/104 - 4s - loss: 0.6669 - accuracy: 0.6918 - val_loss: 1.0596 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "104/104 - 4s - loss: 0.6570 - accuracy: 0.6779 - val_loss: 1.8742 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "104/104 - 4s - loss: 0.6041 - accuracy: 0.7178 - val_loss: 1.9887 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "104/104 - 4s - loss: 0.6669 - accuracy: 0.6737 - val_loss: 1.1722 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "104/104 - 4s - loss: 0.6502 - accuracy: 0.6943 - val_loss: 0.9674 - val_accuracy: 0.0338\n",
      "Epoch 17/30\n",
      "104/104 - 4s - loss: 0.6038 - accuracy: 0.7118 - val_loss: 2.3254 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "104/104 - 4s - loss: 0.6084 - accuracy: 0.7287 - val_loss: 0.3944 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "104/104 - 4s - loss: 0.6359 - accuracy: 0.7009 - val_loss: 1.8815 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "104/104 - 4s - loss: 0.6458 - accuracy: 0.7003 - val_loss: 2.8782 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "104/104 - 4s - loss: 0.6124 - accuracy: 0.7366 - val_loss: 1.2385 - val_accuracy: 0.0048\n",
      "Epoch 22/30\n",
      "104/104 - 4s - loss: 0.6028 - accuracy: 0.7124 - val_loss: 1.7819 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "104/104 - 4s - loss: 0.6494 - accuracy: 0.6973 - val_loss: 1.0557 - val_accuracy: 0.0435\n",
      "Epoch 24/30\n",
      "104/104 - 4s - loss: 0.6307 - accuracy: 0.7233 - val_loss: 0.8738 - val_accuracy: 0.1812\n",
      "Epoch 25/30\n",
      "104/104 - 4s - loss: 0.6521 - accuracy: 0.7015 - val_loss: 1.7856 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "104/104 - 4s - loss: 0.6274 - accuracy: 0.7172 - val_loss: 0.8869 - val_accuracy: 0.1812\n",
      "Epoch 27/30\n",
      "104/104 - 4s - loss: 0.6063 - accuracy: 0.7178 - val_loss: 1.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "104/104 - 4s - loss: 0.5914 - accuracy: 0.7178 - val_loss: 1.9576 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "104/104 - 4s - loss: 0.6253 - accuracy: 0.7069 - val_loss: 2.1336 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "104/104 - 4s - loss: 0.6164 - accuracy: 0.7124 - val_loss: 0.3772 - val_accuracy: 0.9976\n"
     ]
    }
   ],
   "source": [
    "#Fiting the model \n",
    "\n",
    "hist =model.fit(X[train], y[train], validation_split=0.2, epochs=nEpochs, batch_size=nBatch, verbose=2)\n",
    "  #pred = model.predict(X[test]).ravel()\n",
    "\n",
    "  #Calculating Recall and precision\n",
    "  #recall.append( recall_score(y[test], pred.round()) )\n",
    "  #precision.append( precision_score(y[test], pred.round()) )\n",
    "\n",
    "  #ROC AUC\n",
    "  #fpr, tpr, thresholds = roc_curve(y[test], pred)\n",
    "  #tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "  #tprs[-1][0] = 0.0\n",
    "  #roc_auc = auc(fpr, tpr)\n",
    "  #print(roc_auc)\n",
    "  #aucs.append(roc_auc)\n",
    "  #hists_CNN.append(hist)\n",
    "\n",
    "\n",
    "#print(\"Avg AUC:\", np.mean(aucs))\n",
    "\n",
    "#print(\"Avg Recall:\", np.mean(recall))\n",
    "#print(\"Avg Precision:\", np.mean(precision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkbaDnZKZM5B"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, steps=len(test_batches), verbose=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resnet50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
